---
layout: post
title: Model의 Performance Measure
date: 2021-01-08 02:33:23 +0900
category: MachineLearning

---
# Model의 성능 측정

모델의 일반화 성능을 평가할 기준이 필요하며, 이를 Performance Measure 라고 한다.

- 회귀분석에서 가장 자주 사용하는 성능 측정 방법
    - Mean Squared Error (평균 제곱 오차)
        - 학습기의 예측 결과와 정답 데이터를 비교할 때 자주 사용된다
        - <img src = "https://cdn-images-1.medium.com/max/1600/1*3VJyfU1qBqoHwaDJm3KAKA.gif" width="20%">
- 분류 문제에서 자주 사용하는 성능 측정 지표
    - 오차율과 정확도
        - 오차율은 모든 Sample에서 잘못 분류한 Sample 수의 비율
        - 정확도는 모든 Sample에서 제대로 분류한 Sample의 비율
        - 정확도 = 1 - 오차율
    - Recall, Precision, F1 Score
        - Confusion Matrix
            - Binary Classification에서 실제 정답값과 예측값의 조합은 아래와 같은 Confusion Matrix로 표현됨
            - ![](https://miro.medium.com/max/356/1*g5zpskPaxO8uSl0OWT4NTQ.png)
        - Recall과 Precision
            - ![](https://www.researchgate.net/profile/Ibrahim_Gad3/post/What_is_the_best_metric_precision_recall_f1_and_accuracy_to_evaluate_the_machine_learning_model_for_imbalanced_data/attachment/5f0d92695e3fff000177fe28/AS%3A913156859777024%401594724969138/download/accuracy.png)
            - Precision은 등장한 값(양성)들에 대해서 제대로 된 값들의 비율을 의미한다
            - Recall은 등장해야 하는 값들 중에서 제대로 등장한 값들의 비율을 의미한다
            - Recall과 Precision 사이에는 Trade-Off가 존재
                - Recall을 높이기 위해서 검출에 필요한 confidence threshold를 낮춘다면, 검출이 되는 물체들은 많아지겠지만 그만큼 오검출이 될 확률도 높아진다.
                - 즉 양성(Positive)의 수는 많아지지만, 잘못된 양성이 많아지는 것이기 때문에 Precision은 감소한다.
